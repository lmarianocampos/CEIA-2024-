{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TDWOgpJWKQa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Estructura del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEV8WbiWl6k",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "teF9O9JJmG7Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sDBLvbTtlwzs"
   },
   "outputs": [],
   "source": [
    "class ClassEncoder:\n",
    "  def fit(self, y):\n",
    "    self.names = np.unique(y)\n",
    "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
    "    self.fmt = y.dtype\n",
    "    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n",
    "\n",
    "  def _map_reshape(self, f, arr):\n",
    "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
    "    # Q2: por que hace falta un reshape?\n",
    "\n",
    "  def transform(self, y):\n",
    "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
    "\n",
    "  def fit_transform(self, y):\n",
    "    self.fit(y)\n",
    "    return self.transform(y)\n",
    "\n",
    "  def detransform(self, y_hat):\n",
    "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m0KYC8_uSOu4"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    self.encoder = ClassEncoder()\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # first encode the classes\n",
    "    y = self.encoder.fit_transform(y)\n",
    "\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # check that a_priori has the correct number of classes\n",
    "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
    "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IRamFdiGDuSR"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fRtC9HEkO5Hu"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x        \n",
    "\n",
    "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS_zoK-gWkRf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Código para pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz19b6NJed2A"
   },
   "source": [
    "Seteamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m05KrhUDINVs"
   },
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 6543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hkXcoldXOqs",
    "outputId": "2ce8d627-3433-4bdd-d370-85f6b703a7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (150, 4), Y:(150, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "# showing for iris\n",
    "X_full, y_full = get_iris_dataset()\n",
    "\n",
    "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAk-UQCjKecT",
    "outputId": "9566d67a-b78b-4809-bb94-8f605b065db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek data matrix\n",
    "X_full[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdzMURX2KVdO",
    "outputId": "af5fc3ac-b391-4769-de47-44cea4f566c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa']], dtype='<U10')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek target vector\n",
    "y_full[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl8UFh1OegbJ"
   },
   "source": [
    "Separamos el dataset en train y test para medir performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKP_QmWCIECs",
    "outputId": "07798c6a-aa54-430e-d46d-becc2a4315ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 90) (1, 90) (4, 60) (1, 60)\n"
     ]
    }
   ],
   "source": [
    "# preparing data, train - test validation\n",
    "# 70-30 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_transpose(X, y, test_sz, random_state):\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # transpose so observations are column vectors\n",
    "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwgXFPbJemb_"
   },
   "source": [
    "Entrenamos un QDA y medimos su accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dGIf2TA5SpoT"
   },
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Q30DyLWpTL",
    "outputId": "dbccae86-840c-412f-ed97-22cfac21238a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QcLtNNIevC_"
   },
   "source": [
    "Con el magic %%timeit podemos estimar el tiempo que tarda en correr una celda en base a varias ejecuciones. Por poner un ejemplo, acá vamos a estimar lo que tarda un ciclo completo de QDA y también su inferencia (predicción).\n",
    "\n",
    "Ojo! a veces [puede ser necesario ejecutarlo varias veces](https://stackoverflow.com/questions/10994405/python-timeit-results-cached-instead-of-calculated) para obtener resultados consistentes.\n",
    "\n",
    "Si quieren explorar otros métodos de medición también es válido!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnZT-HN2fUuW",
    "outputId": "2618e7c1-7a77-4285-bafb-c2880ad167a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37 ms ± 55.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjFbVSqfeHUX",
    "outputId": "0254a727-a1d5-4be3-b73a-2f55d2c84a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.85 ms ± 42.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model = QDA()\n",
    "model.fit(train_x, train_y)\n",
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Implementación base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribucion del dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_total = QDA()\n",
    "\n",
    "#La probabilidad a priori se basa en las frecuencias de las clases en el conjunto completo.\n",
    "qda_total.fit(X_full.T, y_full.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0200\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "qda_total_acc = accuracy(y_full.T, qda_total.predict(X_full.T))\n",
    "print(f\"Train (apparent) error is {1-qda_total_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribucion uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0222 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_iris, qda_uniform.predict(train_x_iris))\n",
    "test_acc_qda_uniform = accuracy(test_y_iris, qda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Distribuciones sesgadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo nuevo modelo\n",
    "qda_prob = QDA()\n",
    "\n",
    "qda_prob.fit(train_x_iris, train_y_iris, a_priori = [0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0222 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_iris, qda_prob.predict(train_x_iris))\n",
    "test_acc_qda_prob = accuracy(test_y_iris, qda_prob.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio de orden las probabilidades\n",
    "qda_prob.fit(train_x_iris, train_y_iris, a_priori = [0.05, 0.9, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0333 while test error is 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_iris, qda_prob.predict(train_x_iris))\n",
    "test_acc_qda_prob = accuracy(test_y_iris, qda_prob.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambio de orden las probabilidades\n",
    "qda_prob.fit(train_x_iris, train_y_iris, a_priori = [0.05, 0.05, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0333 while test error is 0.0500\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_iris, qda_prob.predict(train_x_iris))\n",
    "test_acc_qda_prob = accuracy(test_y_iris, qda_prob.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA_total | Iris | 6543 | 0.0200 | N/A\n",
    "QDA_uniforme | Iris | 6543 | 0.0222 | 0.0167\n",
    "QDA_sesgada_1 | Iris | 6543 | 0.0222 | 0.0167\n",
    "QDA_sesgada_2 | Iris | 6543 | 0.0333 | 0\n",
    "QDA_sesgada_3 | Iris | 6543 | 0.0333 | 0.0500\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "En cuanto a la diferencia que se observa:\n",
    "- **Distribuciones a priori distintas**, afectan las probabilidades de las clases y, por lo tanto, el comportamiento del modelo. Por ejemplo, si usas una distribución sesgada (donde una clase tiene una probabilidad mucho mayor), el modelo tenderá a clasificar más observaciones en esa clase, lo que podría afectar las métricas de desempeño.\n",
    "- **Cuando spliteas el dataset**, los datos de entrenamiento podrían tener una distribución diferente de clases en comparación con el conjunto completo, lo que podría hacer que las distribuciones a priori aprendidas por el modelo sean diferentes si no se establecen explícitamente.\n",
    "- **Cuando no se splitea el dataset**, esto debería dar como resultado distribuciones a priori basadas en las frecuencias de las clases en el conjunto completo, por ende el las metricas de desempeño aumentan porque las distribuciones a priori son mas acorde con la distribucion de clases en el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Distribucion del dataset completo de penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seteamos penguin\n",
    "X_full, y_full = get_penguins()\n",
    "\n",
    "#Creo el modelo\n",
    "qda_total = QDA()\n",
    "\n",
    "#La probabilidad a priori se basa en las frecuencias de las clases en el conjunto completo.\n",
    "qda_total.fit(X_full.T, y_full.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0117\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "qda_total_acc = accuracy(y_full.T, qda_total.predict(X_full.T))\n",
    "print(f\"Train (apparent) error is {1-qda_total_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribucion uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset penguin\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_penguins, train_y_penguins, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0098 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_penguins, qda_uniform.predict(train_x_penguins))\n",
    "test_acc_qda_uniform = accuracy(test_y_penguins, qda_uniform.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuciones sesgadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo nuevo modelo\n",
    "qda_prob = QDA()\n",
    "\n",
    "qda_prob.fit(train_x_penguins, train_y_penguins, a_priori = [0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0195 while test error is 0.0219\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_penguins, qda_prob.predict(train_x_penguins))\n",
    "test_acc_qda_prob = accuracy(test_y_penguins, qda_prob.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_prob.fit(train_x_penguins, train_y_penguins, a_priori = [0.05, 0.9, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0098 while test error is 0.0219\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_penguins, qda_prob.predict(train_x_penguins))\n",
    "test_acc_qda_prob = accuracy(test_y_penguins, qda_prob.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_prob.fit(train_x_penguins, train_y_penguins, a_priori = [0.05, 0.05, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0098 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_prob = accuracy(train_y_penguins, qda_prob.predict(train_x_penguins))\n",
    "test_acc_qda_prob = accuracy(test_y_penguins, qda_prob.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_prob:.4f} while test error is {1-test_acc_qda_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA_total | Penguins | 6543 | 0.0117 | N/A\n",
    "QDA_uniforme | Penguins | 6543 | 0.0098 | 0.0073\n",
    "QDA_sesgada_1 | Penguins | 6543 | 0.0195 | 0.0219\n",
    "QDA_sesgada_2 | Penguins | 6543 | 0.0098 | 0.0219\n",
    "QDA_sesgada_3 | Penguins | 6543 | 0.0098 | 0.0073\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "**Conclusión:**\n",
    "- El **error aparente en el conjunto de entrenamiento** generalmente será bajo, pero lo importante es cómo se comporta el modelo en el conjunto de test. Si las distribuciones a priori son muy sesgadas (como [0.9, 0.05, 0.05]), el modelo puede tender a favorecer más una clase y perder precisión en otras, lo que lleva a un mayor **error de test**.\n",
    "- Las distribuciones a priori balanceadas (uniformes o moderadamente sesgadas) parecen producir los mejores resultados en este dataset.\n",
    "  \n",
    "El comportamiento de los errores refleja cómo el modelo ajusta las probabilidades a priori y cómo las clases desbalanceadas afectan las predicciones. Si alguna clase está sobrerrepresentada, como en el caso de la clase con probabilidad 0.9, el modelo podría obtener buenos resultados en el entrenamiento pero no generalizar bien a nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Implementacion del modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(BaseBayesianClassifier):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # Calcular la matriz de covarianza de cada clase (Σ_j)\n",
    "        self.cov_matrices = [np.cov(X[:, y.flatten() == idx], bias=True)\n",
    "                             for idx in range(len(self.log_a_priori))]\n",
    "        \n",
    "        # Calcular el promedio ponderado para la matriz de covarianza global (Σ)\n",
    "        class_sizes = [np.sum(y.flatten() == idx) for idx in range(len(self.log_a_priori))]\n",
    "        n_total = len(y.flatten())\n",
    "        self.cov_matrix = sum((class_sizes[idx] / n_total) * self.cov_matrices[idx]\n",
    "                              for idx in range(len(self.log_a_priori)))\n",
    "        \n",
    "        # Calcular la inversa de la matriz de covarianza global\n",
    "        self.inv_cov_matrix = inv(self.cov_matrix)\n",
    "        \n",
    "        # Calcular las medias de cada clase (μ_j)\n",
    "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    #def _predict_log_conditional(self, x, class_idx):\n",
    "        # Calcular el logaritmo de P(x|G=class_idx), el log de la probabilidad condicional\n",
    "       # unbiased_x = x - self.means[class_idx]\n",
    "      #  return -0.5 * np.log(det(self.cov_matrix)) - 0.5 * unbiased_x.T @ self.inv_cov_matrix @ unbiased_x\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        # x debe ser un vector columna\n",
    "        inv_cov = self.inv_cov_matrix\n",
    "        mean = self.means[class_idx]\n",
    "    \n",
    "        # Calcular el término lineal\n",
    "        linear_term = mean.T @ inv_cov @ x\n",
    "    \n",
    "        # Calcular el término cuadrático constante para la clase\n",
    "        class_constant = -0.5 * mean.T @ inv_cov @ mean\n",
    "    \n",
    "        return (linear_term + class_constant).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribucion del dataset completo de iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, y_full = get_iris_dataset()\n",
    "#Creo el modelo\n",
    "lda_total = LDA()\n",
    "\n",
    "#La probabilidad a priori se basa en las frecuencias de las clases en el conjunto completo.\n",
    "lda_total.fit(X_full.T, y_full.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0200\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "lda_total_acc = accuracy(y_full.T, lda_total.predict(X_full.T))\n",
    "print(f\"Train (apparent) error is {1-lda_total_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "lda_uniform = LDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "lda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0222 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribucion del dataset completo de penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, y_full = get_penguins()\n",
    "#Creo el modelo\n",
    "lda_total = LDA()\n",
    "\n",
    "#La probabilidad a priori se basa en las frecuencias de las clases en el conjunto completo.\n",
    "lda_total.fit(X_full.T, y_full.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0117\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "lda_total_acc = accuracy(y_full.T, lda_total.predict(X_full.T))\n",
    "print(f\"Train (apparent) error is {1-lda_total_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "lda_uniform = LDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "lda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0098 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA_total | Iris | 6543 | 0.0200 | N/A\n",
    "QDA_uniforme | Iris | 6543 | 0.0222 | 0.0167\n",
    "QDA_total | Penguins | 6543 | 0.0117 | N/A\n",
    "QDA_uniforme | Penguins | 6543 | 0.0098 | 0.0073\n",
    "LDA_total | Iris | 6543 | 0.0200 | N/A\n",
    "LDA_uniforme | Iris | 6543 | 0.0222 | 0.0167\n",
    "LDA_total | Penguins | 6543 | 0.0117 | N/A\n",
    "LDA_uniforme | Penguins | 6543 | 0.0098 | 0.0073\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "**Los resultados son consistentes con datasets como Iris y Penguins, donde las clases están bien definidas y no tienen grandes diferencias en sus varianzas. Esto demuestra que ambos modelos funcionan bien, pero LDA puede considerarse mejor en este caso por ser más simple.** (verificar)\n",
    "\n",
    "No se observan diferencias en los resultados pero podria decirse que LDA es notoriamente mejor por la robustes del metodo que requiere menos parametros para su implementacion. La ventaja de QDA quizas no se esta aprovechando porque las diferencias en las varianzas de las clases no deben ser grandes, entonces la generalizacion que hace LDA lo convierte en el metodo de preferencia por ser mas simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rng_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme QDA iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_iris, qda_uniform.predict(train_x_iris))\n",
    "test_acc_qda_uniform = accuracy(test_y_iris, qda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme LDA iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "lda_uniform = LDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "lda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0000 while test error is 0.0667\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme QDA penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_penguins, train_y_penguins, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0049 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_penguins, qda_uniform.predict(train_x_penguins))\n",
    "test_acc_qda_uniform = accuracy(test_y_penguins, qda_uniform.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme LDA penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "lda_uniform = LDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "lda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0146 while test error is 0.0073\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rng_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme QDA iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0833\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_iris, qda_uniform.predict(train_x_iris))\n",
    "test_acc_qda_uniform = accuracy(test_y_iris, qda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme LDA iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "lda_uniform = LDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "lda_uniform.fit(train_x_iris, train_y_iris, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0000 while test error is 0.0500\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme QDA penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo el modelo\n",
    "qda_uniform = QDA()\n",
    "\n",
    "#La probabilidad a priori es un array de largo 3 porque CLass encoder toma los y unicos, \n",
    "#en este caso son 3\n",
    "qda_uniform.fit(train_x_penguins, train_y_penguins, a_priori = [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0098 while test error is 0.0146\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_qda_uniform = accuracy(train_y_penguins, qda_uniform.predict(train_x_penguins))\n",
    "test_acc_qda_uniform = accuracy(test_y_penguins, qda_uniform.predict(test_x_penguins))\n",
    "print(f\"Train (apparent) error is {1-train_acc_qda_uniform:.4f} while test error is {1-test_acc_qda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion uniforme LDA penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el dataset iris\n",
    "X_iris, y_iris = get_penguins()\n",
    "\n",
    "#Separo en test de entrenamiento y test de evaluacio\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 1.0000 while test error is 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Mido prescicion del modelo\n",
    "train_acc_lda_uniform = accuracy(train_y_iris, lda_uniform.predict(train_x_iris))\n",
    "test_acc_lda_uniform = accuracy(test_y_iris, lda_uniform.predict(test_x_iris))\n",
    "print(f\"Train (apparent) error is {1-train_acc_lda_uniform:.4f} while test error is {1-test_acc_lda_uniform:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA_uniforme | Iris | 3000 | 0.0111 | 0.0333\n",
    "LDA_uniforme | Iris | 3000 | 0 | 0.0667\n",
    "QDA_uniforme | penguins | 3000 | 0.0049 | 0.0073\n",
    "LDA_uniforme | penguins | 3000 | 0.0146 | 0.0073\n",
    "QDA_uniforme | Iris | 2000 | 0.0111 | 0.0833\n",
    "LDA_uniforme | Iris | 2000 | 0 | 0.0500\n",
    "QDA_uniforme | penguins | 2000 | 0.0098 | 0.0146\n",
    "LDA_uniforme | penguins | 2000 | 0.0098 | 0.0146\n",
    "\n",
    "</center>\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- QDA tiene mejor desempeño en el dataset Iris (menor error de prueba) en la mayoría de los casos, pero es más sensible a los splits de datos y puede sobreajustarse si las matrices de covarianza de las clases no son muy distintas.\n",
    "- LDA es más estable y consistente, especialmente en el dataset Penguins, donde las clases tienen propiedades similares. También tiene menos riesgo de sobreajuste debido a su simplicidad.\n",
    "\n",
    "Ninguno de los dos modelos es estrictamente \"mejor\" en todos los escenarios. La elección entre QDA y LDA debe depender de las características del dataset y del propósito del análisis. En general, si las clases tienen matrices de covarianza similares, LDA es más apropiado.\n",
    "Si las clases son más heterogéneas, QDA puede ofrecer ventajas si el dataset tiene suficientes datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "qda_timeit = QDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "qda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37 ms ± 119 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "qda_timeit = QDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "qda_timeit.fit(train_x_penguins, train_y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.68 ms ± 65 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda_timeit.predict(test_x_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "tensorizedqda_timeit = TensorizedQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "tensorizedqda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.71 ms ± 13.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "tensorizedqda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "tensorizedqda_timeit = TensorizedQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "tensorizedqda_timeit.fit(train_x_penguins, train_y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64 ms ± 24.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "tensorizedqda_timeit.predict(test_x_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Mean [ms] | ± Std [us] | Parametro | Runs/Loops\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 4000 | 4.41 | 52.4 | p | 7/100\n",
    "QDA | Penguins | 4000 | 10.3 | 316 | p | 7/100\n",
    "TensorizedQDA | Iris | 4000 | 1.66 | 17.6 | p | 7/100\n",
    "TensorizedQDA | Penguins | 4000 | 3.56 | 193 | p | 7/100\n",
    "</center>\n",
    "\n",
    "**Observaciones**\n",
    "TensorizedQDA es significativamente más rápido que QDA para la predicción, con tiempos promedios que son aproximadamente 2.5x a 3x menores, dependiendo del dataset. Esto se debe a su uso de operaciones vectorizadas, que optimizan los cálculos al reducir redundancias y aprovechar mejor el hardware moderno, como las tarjetas graficas. Ambos métodos producen los mismos resultados, pero TensorizedQDA es preferible cuando se prioriza la eficiencia computacional. Se destaca el hecho de que la optimizacion no se da a nivel de codigo sino a nivel matematico, trabajando con tensores en tres dimensiones en lugar de matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizacion matematica QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \n",
    "    def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "        m_obs = X.shape[1]\n",
    "        p_par = X.shape[0]\n",
    "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "        \n",
    "        new_tensor_means = np.repeat(self.tensor_means, m_obs, axis=2) #Forma(clases,p,n)\n",
    "        unbiased_x = X - new_tensor_means\n",
    "\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x #Forma(clases,nxn)\n",
    "        #print(inner_prod.shape)\n",
    "        #diagonal = inner_prod.diagonal()\n",
    "        diagonal = np.array([inner_prod[i].diagonal() for i in range(inner_prod.shape[0])]).T  # Forma (n,clases)\n",
    "        #print(diagonal.shape)\n",
    "        log_dets = 0.5*np.log(det(self.tensor_inv_cov))\n",
    "        \n",
    "        log_posteriors = self.log_a_priori + log_dets - 0.5 * diagonal\n",
    "       \n",
    "        y_hat_indices = np.argmax(log_posteriors, axis=1)\n",
    "        y_hat = self.encoder.detransform(y_hat_indices)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "fasterqda_timeit = FasterQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "fasterqda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 μs ± 1.66 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "fasterqda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "fasterqda_timeit = FasterQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "fasterqda_timeit.fit(train_x_penguins, train_y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 μs ± 7.56 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "fasterqda_timeit.predict(test_x_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "Modelo | Dataset | Seed | Mean [ms] | ± Std [us] | Parametro | Runs/Loops\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 4000 | 4.41 | 52.4 | p | 7/100\n",
    "TensorizedQDA | Iris | 4000 | 1.66 | 17.6 | p | 7/100\n",
    "FasterQDA | Iris | 4000 | 0.121 | 1.66 | 100 | 7/100\n",
    "QDA | Penguins | 4000 | 10.3 | 316 | p | 7/100\n",
    "TensorizedQDA | Penguins | 4000 | 3.56 | 193 | p | 7/100\n",
    "FasterQDA | Penguins | 4000 | 0.184 | 7.56 | 100 | 7/100\n",
    "</center>\n",
    "\n",
    "**Conclusiones**\n",
    "\n",
    "- Para el dataset de Iris: FasterQDA es aproximadamente 36 veces más rápido que QDA tradicional y 14 veces más rápido que TensorizedQDA. La variabilidad en tiempos de predicción es mucho menor para FasterQDA, lo que demuestra un desempeño más estable.\n",
    "- Para el dataset de Penguins: FasterQDA es aproximadamente 56 veces más rápido que QDA tradicional y 20 veces más rápido que TensorizedQDA. Nuevamente, FasterQDA tiene la menor variabilidad, lo que lo hace ideal para aplicaciones sensibles al tiempo.\n",
    "\n",
    "En resumen, FasterQDA es extremadamente eficiente en comparación con QDA tradicional y TensorizedQDA. Esto se debe a que reduce drásticamente los cálculos internos y optimiza las operaciones relacionadas con las diagonales y los productos internos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    \n",
    "    def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "        m_obs = X.shape[1]\n",
    "        p_par = X.shape[0]\n",
    "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "        \n",
    "        new_tensor_means = np.repeat(self.tensor_means, m_obs, axis=2) #Forma(clases,p,n)\n",
    "        unbiased_x = X - new_tensor_means\n",
    "\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x #Forma(clases,nxn)\n",
    "        print(\"La matriz nxn es la que esta en el tensor (clases, nxn)\", inner_prod.shape)\n",
    "        diagonal = np.array([inner_prod[i].diagonal() for i in range(inner_prod.shape[0])]).T  # Forma (n,clases)\n",
    "        #print(diagonal.shape)\n",
    "        log_dets = 0.5*np.log(det(self.tensor_inv_cov))\n",
    "        \n",
    "        log_posteriors = self.log_a_priori + log_dets - 0.5 * diagonal\n",
    "       \n",
    "        y_hat_indices = np.argmax(log_posteriors, axis=1)\n",
    "        y_hat = self.encoder.detransform(y_hat_indices)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matriz nxn es la que esta en el tensor (clases, nxn) (3, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "fasterqda_timeit = FasterQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "fasterqda_timeit.fit(train_x_iris, train_y_iris)\n",
    "\n",
    "fasterqda_timeit.predict(test_x_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Demostración de la Propiedad Matemática:**\n",
    "\n",
    "La diagonal del producto de dos matrices $ A \\cdot B $ es igual a la suma por filas del producto elemento a elemento de $ A $ y la transpuesta de $ B $:\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\sum_{\\text{columnas}} A \\odot B^T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ A $ es una matriz de dimensiones $ n \\times p $.\n",
    "- $ B $ es una matriz de dimensiones $ p \\times n $.\n",
    "- $ \\odot $ representa el producto elemento a elemento (producto de Hadamard).\n",
    "\n",
    "---\n",
    "\n",
    "Demostración:\n",
    "#### Elemento $ ii $ de la Diagonal:\n",
    "\n",
    "El elemento en la posición $ ii $ de la diagonal del producto $ A \\cdot B $ es:\n",
    "$$\n",
    "(A \\cdot B)_{ii} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{ki}\n",
    "$$\n",
    "Es decir, es la suma sobre $ k $ de $ A_{ik} $ multiplicado por $ B_{ki} $.\n",
    "\n",
    "---\n",
    "\n",
    "#### Producto Elemento a Elemento con $ B^T $:\n",
    "\n",
    "Al considerar la transpuesta de $ B $, es decir, $ B^T $, que es de dimensiones $ n \\times p $, el producto elemento a elemento $ A \\odot B^T $ resulta en una matriz $ C $ de dimensiones $ n \\times p $, donde cada elemento es:\n",
    "$$\n",
    "C_{ik} = A_{ik} \\cdot B_{ik}^T = A_{ik} \\cdot B_{ki}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Suma por Filas:\n",
    "\n",
    "La suma de los elementos de la fila $ i $ de $ C $ es:\n",
    "$$\n",
    "\\sum_{k=1}^{p} C_{ik} = \\sum_{k=1}^{p} A_{ik} \\cdot B_{ki}\n",
    "$$\n",
    "Que es exactamente el elemento $ ii $ de la diagonal de $ A \\cdot B $.\n",
    "\n",
    "---\n",
    "\n",
    "Conclusión:\n",
    "Por lo tanto:\n",
    "$$\n",
    "\\text{diag}(A \\cdot B) = \\sum_{\\text{columnas}} A \\odot B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvenFasterQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "        m_obs = X.shape[1]\n",
    "        p_par = X.shape[0]\n",
    "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "        \n",
    "        new_tensor_means = np.repeat(self.tensor_means, m_obs, axis=2) #Forma(clases,p,n)\n",
    "        unbiased_x = X - new_tensor_means\n",
    "\n",
    "        A = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov\n",
    "        B = unbiased_x\n",
    "        \n",
    "        diagonal = np.sum(A * B.transpose(0,2,1), axis = 2).T  # Forma (n,clases)\n",
    "        log_dets = 0.5*np.log(det(self.tensor_inv_cov))\n",
    "        \n",
    "        log_posteriors = self.log_a_priori + log_dets - 0.5 * diagonal\n",
    "       \n",
    "        y_hat_indices = np.argmax(log_posteriors, axis=1)\n",
    "        y_hat = self.encoder.detransform(y_hat_indices)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "evenfasterqda_timeit = EvenFasterQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "evenfasterqda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 μs ± 16.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "evenfasterqda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "evenfasterqda_timeit = EvenFasterQDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "evenfasterqda_timeit.fit(train_x_penguins, train_y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 μs ± 3.26 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "evenfasterqda_timeit.predict(test_x_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "Modelo | Dataset | Seed | Mean [ms] | ± Std [us] | Parametro | Runs/Loops\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 4000 | 4.41 | 52.4 | 100 | 7/100\n",
    "TensorizedQDA | Iris | 4000 | 1.66 | 17.6 | 100 | 7/100\n",
    "FasterQDA | Iris | 4000 | 0.121 | 1.66 | 100 | 7/100\n",
    "EvenFasterQDA | Iris | 4000 | 0.140 | 16.3 | 100 | 7/100\n",
    "QDA | Penguins | 4000 | 10.3 | 316 | 100 | 7/100\n",
    "TensorizedQDA | Penguins | 4000 | 3.56 | 193 | 100 | 7/100\n",
    "FasterQDA | Penguins | 4000 | 0.184 | 7.56 | 100 | 7/100\n",
    "EvenFasterQDA | Penguins | 4000 | 0.135 | 3.26 | 100 | 7/100\n",
    "\n",
    "</center>\n",
    "\n",
    "**Conclusiones**\n",
    "\n",
    "- Para el dataset de Iris: EvenFasterQDA tiene un tiempo de predicción ligeramente mayor que FasterQDA (0.140 ms vs. 0.121 ms). Sin embargo, esta diferencia es marginal y probablemente no significativa en la práctica. La variabilidad en los tiempos de EvenFasterQDA es mayor que en FasterQDA (± 16.3 µs vs. ± 1.66 µs), lo que indica una posible pérdida de estabilidad en esta nueva implementación.\n",
    "- Para el dataset de Penguins: EvenFasterQDA logra una mejora marginal en el tiempo promedio de predicción frente a FasterQDA (0.135 ms vs. 0.184 ms), siendo aproximadamente 1.36 veces más rápido. La variabilidad también se reduce significativamente (± 3.26 µs vs. ± 7.56 µs), lo que sugiere mayor estabilidad en esta implementación para este dataset.\n",
    "\n",
    "En resumen, EvenFasterQDA logra una ligera mejora en el tiempo promedio de predicción frente a FasterQDA en el dataset Penguins. Sin embargo, en el dataset Iris, los tiempos de predicción son ligeramente mayores. Las diferencias son pequeñas, por lo que, en términos de velocidad, las implementaciones FasterQDA y EvenFasterQDA son prácticamente equivalentes.\n",
    "\n",
    "En el dataset Penguins, EvenFasterQDA muestra una mejora significativa en la estabilidad (menor variabilidad). Sin embargo, en el dataset Iris, la variabilidad aumenta respecto a FasterQDA, lo que sugiere que la implementación no es consistentemente mejor en todos los casos.\n",
    "\n",
    "La propiedad utilizada en EvenFasterQDA introduce una nueva forma de optimizar los cálculos, pero no siempre conduce a mejoras significativas en el tiempo de predicción. Esto sugiere que la implementación FasterQDA ya estaba cerca del límite de optimización en este contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizacion matematica LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedLDA(LDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_cov_matrix)#inv(self.tensor_cov_matrix)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "        self.tensor_means_x_inv_cov = self.tensor_means.transpose(0,2,1) @ self.tensor_inv_cov\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "         # Calcular el término lineal\n",
    "        linear_term = self.tensor_means_x_inv_cov @ x\n",
    "    \n",
    "        # Calcular el término cuadrático constante para la clase\n",
    "        class_constant = -0.5 * self.tensor_means_x_inv_cov @ self.tensor_means\n",
    "        return (linear_term + class_constant).flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo los modelos\n",
    "lda_timeit = LDA()\n",
    "tensorizedlda_timeit = TensorizedLDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "lda_timeit.fit(train_x_iris, train_y_iris)\n",
    "tensorizedlda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.59 ms ± 38.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "lda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826 μs ± 19.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "tensorizedlda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo los modelos\n",
    "lda_timeit = LDA()\n",
    "tensorizedlda_timeit = TensorizedLDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "lda_timeit.fit(train_x_iris, train_y_iris)\n",
    "tensorizedlda_timeit.fit(train_x_penguins, train_y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 ms ± 36.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "lda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 ms ± 38.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "tensorizedlda_timeit.predict(test_x_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Mean [ms] | ± Std [us] | Parametro | Runs/Loops\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "LDA | Iris | 4000 | 2.59 | 38.3 | 100 | 7/100\n",
    "TensorizedLDA | Iris | 4000 | 0.826 | 19.1 | 100 | 7/100\n",
    "LDA | Penguins | 4000 | 2.62 | 36.7 | 100 | 7/100\n",
    "TensorizedLDA | Penguins | 4000 | 1.69 | 38.9 | 100 | 7/100\n",
    "</center>\n",
    "\n",
    "**Conclusiones**\n",
    "\n",
    "- Para el dataset de Iris: el modelo tensorizado muestra una mejora significativa en el tiempo de predicción, siendo aproximadamente 3 veces más rápido que el modelo LDA tradicional. Además, tiene una menor variabilidad en el tiempo de predicción, lo que sugiere mayor estabilidad.\n",
    "- Para el dataset de Penguins: en este caso, la versión tensorizada también supera al modelo LDA tradicional, siendo aproximadamente 1.5 veces más rápida. Sin embargo, la variabilidad en los tiempos de predicción es ligeramente mayor en el modelo tensorizado.\n",
    "\n",
    "En resumen, el aprovechamiento del calculo matricial en la version tensorizada reduce la necesidad de realizar cálculos redundantes lo que mejora la eficiencia y en cuanto a la estabilidad, si bien la version tensorizada es mas rapida, no necesariamente es mas estable en todos los datasets como se observa en Penguins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Faster LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterLDA(TensorizedLDA):\n",
    "    def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "        m_obs = X.shape[1]\n",
    "        p_par = X.shape[0]\n",
    "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "        \n",
    "        new_tensor_means = np.repeat(self.tensor_means, m_obs, axis=2) #Forma(clases,p,n)\n",
    "        self.tensor_means_x_inv_cov = new_tensor_means.transpose(0,2,1) @ self.tensor_inv_cov\n",
    "        \n",
    "        linear_term = self.tensor_means_x_inv_cov @ X\n",
    "        # Calcular el término cuadrático constante para la clase\n",
    "        class_constant = -0.5 * self.tensor_means_x_inv_cov @ new_tensor_means\n",
    "        log_cond = (linear_term + class_constant)\n",
    "        \n",
    "        diagonal = np.array([log_cond[i].diagonal() for i in range(log_cond.shape[0])]).T\n",
    "        log_posteriors = self.log_a_priori + diagonal\n",
    "       \n",
    "        y_hat_indices = np.argmax(log_posteriors, axis=1)\n",
    "        y_hat = self.encoder.detransform(y_hat_indices)\n",
    "        return y_hat.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_iris, y_iris = get_iris_dataset()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_iris, train_y_iris, test_x_iris, test_y_iris = split_transpose(X_iris, y_iris, 0.4, rng_seed)\n",
    "#Creo el modelo\n",
    "fasterlda_timeit = FasterLDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "fasterlda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 μs ± 8.71 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "fasterlda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed para split\n",
    "rng_seed = 4000\n",
    "#cargamos los dataset nuevamente\n",
    "X_penguins, y_penguins = get_penguins()\n",
    "#Separo en test de entrenamiento y test de evaluacion\n",
    "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins, y_penguins, 0.4, rng_seed)\n",
    "#Creo el model\n",
    "fasterlda_timeit = FasterLDA()\n",
    "#La probabilidad a priori frecuentista\n",
    "fasterlda_timeit.fit(train_x_iris, train_y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 μs ± 11 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "\n",
    "fasterlda_timeit.predict(test_x_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Mean [ms] | ± Std [us] | Parametro | Runs/Loops\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "LDA | Iris | 4000 | 2.59 | 38.3 | 100 | 7/100\n",
    "TensorizedLDA | Iris | 4000 | 0.826 | 19.1 | 100 | 7/100\n",
    "FasterLDA | Iris | 4000 | 0.126 | 8.71 | 100 | 7/100\n",
    "LDA | Penguins | 4000 | 2.62 | 36.7 | 100 | 7/100\n",
    "TensorizedLDA | Penguins | 4000 | 1.69 | 38.9 | 100 | 7/100\n",
    "FasterLDA | Penguins | 4000 | 0.133 | 11 | 100 | 7/100\n",
    "</center>\n",
    "\n",
    "**Conclusiones**\n",
    "\n",
    "- Para el dataset de Iris: FasterLDA es aproximadamente 20 veces más rápido que LDA tradicional y 6.5 veces más rápido que TensorizedLDA en este dataset. También tiene la menor variabilidad en tiempos de predicción, lo que indica un desempeño muy estable.\n",
    "- Para el dataset de Penguins: FasterLDA es aproximadamente 20 veces más rápido que LDA tradicional y 13 veces más rápido que TensorizedLDA en este dataset. La variabilidad en tiempos de predicción es nuevamente la más baja, lo que demuestra su estabilidad y eficiencia.\n",
    "\n",
    "En resumen, FasterLDA supera ampliamente a las implementaciones anteriores, logrando tiempos de predicción significativamente menores en ambos datasets. Esto se debe a que evita cálculos innecesarios y aprovecha mejor las operaciones matriciales en batch. Además de ser más rápido, FasterLDA muestra menor variabilidad en los tiempos de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ejercicio teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder calcular las derivadas parciales de $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$ respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$, teniendo en cuentas las observaciones de $x$ e $y$, debemos realizar primero foward propagation y luego back propagation.\n",
    "Para poder facilitar los procedimientos y calculos nobrareos a las siguientes variables:\n",
    "$$\n",
    "z^{(1)} = w^{(1)} \\cdot x+b^{(1)}\n",
    "$$\n",
    "$$\n",
    "y^{(1)} = \\sigma (z^{(1)})\n",
    "$$\n",
    "$$\n",
    "z^{(2)} = w^{(2)} \\cdot y^{(1)} +b^{(2)}\n",
    "$$\n",
    "$$\n",
    "y^{(2)} = \\sigma (z^{(2)})\n",
    "$$\n",
    "$$\n",
    "\\hat y = y^{(2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos de calcular $\\hat y$ en base a los datos conocidos. Para ello el orden a seguir es calcular: \n",
    "1. $z^{(1)}$\n",
    "2. $y^{(1)}$\n",
    "3. $z^{(2)}$\n",
    "4. $y^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creaos los parametros\n",
    "w_1 = np.array([[0.1, -0.5], [-0.3, -0.9], [0.8, 0.02]])\n",
    "\n",
    "b_1 = np.array([[0.1, 0.5, 0.8]]).T\n",
    "\n",
    "w_2 = np.array([[-0.4, 0.2, -0.5]])\n",
    "\n",
    "b_2 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos las observaciones\n",
    "x = np.array([[1.8, -3.4]]).T\n",
    "\n",
    "y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la funcion z_i\n",
    "def z_i(w, x, b):\n",
    "    return w @ x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la funcion z_i\n",
    "def y_i(z_i):\n",
    "    return 1/(1+np.exp(-z_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos $z^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.98 ],\n",
       "       [3.02 ],\n",
       "       [2.172]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_1 = z_i(w_1, x, b_1)\n",
    "z_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "1.98 \\\\\n",
    "3.02 \\\\\n",
    "2.172\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos $y^{(1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87868116],\n",
       "       [0.95346953],\n",
       "       [0.89770677]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = y_i(z_1)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y^{(1)} = \n",
    "\\begin{pmatrix}\n",
    "0.87868116 \\\\\n",
    "0.95346953 \\\\\n",
    "0.89770677\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos $z^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09036805]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_2 = z_i(w_2, y_1, b_2)\n",
    "z_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{(2)} = 0.09036805\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos $y^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52257665]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2 = y_i(z_2)\n",
    "y_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y^{(2)} = 0.52257665\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto $\\hat y = 0.52257665$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo los resultados calculados anteriormente, ahora debemos hallar las derivadas parciales de cada $J(\\theta)$ respecto a cada uno de los parametros.\n",
    "Para ello comenzamos planteando:\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Donde conocemos tanto $\\hat{y}$ como $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.47742335]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_y_techo = y_2 - y\n",
    "derivada_j_respecto_y_techo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} - y = -4.47742335\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien resulta importante calcular $\\frac{\\partial J(\\theta)}{\\partial z^{(2)}}$\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial z^{(2)}} =  \\frac{\\partial J(\\theta)}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial z^{(2)}}\n",
    "$$\n",
    "Donde podemos experesar $\\hat y = \\sigma (z^{(2)})$\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial z^{(2)}} =  \\frac{\\partial J(\\theta)}{\\partial \\hat y} \\frac{\\partial \\sigma (z^{(2)})}{\\partial z^{(2)}}\n",
    "$$\n",
    "Donde $\\frac{\\partial \\sigma (z^{(2)})}{\\partial z^{(2)}}$ es la derivada de la funcion sigmoidea la cual es $\\sigma (z^{(2)})(1-\\sigma (z^{(2)}))$\n",
    "Por lo tanto nos queda:\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial z^{(2)}} = (\\hat y - y)(z^{(2)})(1-\\sigma (z^{(2)}))\n",
    "$$\n",
    "o \n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial z^{(2)}} = (\\hat y - y)(\\hat y)(1-\\hat y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.11707367]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_z_2 = derivada_j_respecto_y_techo * y_2 * (1 - y_2)\n",
    "derivada_j_respecto_z_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial z^{(2)}} = -1.11707367\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada respecto a $b^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamo planteando\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial b^{(2)}} = \\frac{\\partial J(\\theta)}{\\partial z^{(2)}} \\frac {\\partial z^{(2)}}{\\partial b^{(2)}}\n",
    "$$\n",
    "Donde $\\frac{\\partial J(\\theta)}{\\partial z^{(2)}}$ ya lo conocemos y $\\frac {\\partial z^{(2)}}{\\partial b^{(2)}}$ es igual a 1. Entonces:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial b^{(2)}} = -1.11707367\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada respecto a $w^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planteamos inicilamente:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(2)}} = \\frac{\\partial J(\\theta)}{\\partial z^{(2)}} \\frac {\\partial z^{(2)}}{\\partial w^{(2)}}\n",
    "$$\n",
    "Donde:\n",
    "$$\n",
    "\\frac {\\partial z^{(2)}}{\\partial w^{(2)}} =y^{(1)}\n",
    "$$\n",
    "Quedando entonces:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(2)}} = \\frac{\\partial J(\\theta)}{\\partial z^{(2)}} y^{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.98155159],\n",
       "       [-1.0650957 ],\n",
       "       [-1.0028046 ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_w_2 = derivada_j_respecto_z_2 * y_1\n",
    "derivada_j_respecto_w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(2)}} = \\begin{pmatrix}\n",
    "-0.98155159 \\\\\n",
    "-1.0650957 \\\\\n",
    "-1.0028046\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada respecto a $b^{(1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planteamos:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial b^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial z^{(1)}} \\frac {\\partial z^{(1)}}{\\partial b^{(1)}}\n",
    "$$\n",
    "Donde para poder realizar el calculo debemos hallar primero $\\frac {\\partial J(\\theta)}{\\partial z^{(1)}}$\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial z^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial y^{(1)}} \\frac {\\partial y^{(1)}}{\\partial z^{(1)}}\n",
    "$$\n",
    "Donde tambien es necesario calcular $\\frac {\\partial J(\\theta)}{\\partial y^{(1)}}$\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial y^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial z^{(2)}} \\frac {\\partial z^{(2)}}{\\partial y^{(1)}}\n",
    "$$\n",
    "Donde:\n",
    "$$\n",
    "\\frac {\\partial z^{(2)}}{\\partial y^{(1)}} = w^{(2)}\n",
    "$$\n",
    "Tenemos entonces:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial y^{(1)}} = (w^{(2)})^T \\frac {\\partial J(\\theta)}{\\partial z^{(2)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44682947],\n",
       "       [-0.22341473],\n",
       "       [ 0.55853684]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_y_1 =  w_2.T * derivada_j_respecto_z_2\n",
    "derivada_j_respecto_y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial y^{(1)}} = \\begin{pmatrix}\n",
    "0.44682947 \\\\\n",
    "-0.22341473 \\\\\n",
    "0.55853684\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo $\\frac {\\partial J(\\theta)}{\\partial y^{(1)}}$ podemos calcular $\\frac {\\partial J(\\theta)}{\\partial z^{(1)}}$\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial z^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial y^{(1)}} \\frac {\\partial y^{(1)}}{\\partial z^{(1)}}\n",
    "$$\n",
    "Donde podemos planntear $y^{(1)}$ como $\\sigma (z^{(1)})$, quedando nuevamente la derivada de la funcion sigmoidea\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial z^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial y^{(1)}} \\sigma (z^{(1)}) (1 - \\sigma (z^{(1)}))\n",
    "$$\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial z^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial y^{(1)}} (y^{(1)}) (1 - y^{(1)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04763228],\n",
       "       [-0.00991188],\n",
       "       [ 0.05129006]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_z_1 = derivada_j_respecto_y_1 * y_1 * (1 - y_1)\n",
    "derivada_j_respecto_z_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial z^{(1)}} = \\begin{pmatrix}\n",
    "0.04763228 \\\\\n",
    "-0.00991188 \\\\\n",
    "0.05129006\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si estamos en condiciones de calcular:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial b^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial z^{(1)}} \\frac {\\partial z^{(1)}}{\\partial b^{(1)}}\n",
    "$$\n",
    "Donde $\\frac {\\partial z^{(1)}}{\\partial b^{(1)}}$ es igual a 1. Por lo tanto: \n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial b^{(1)}} = \\begin{pmatrix}\n",
    "0.04763228 \\\\\n",
    "-0.00991188 \\\\\n",
    "0.05129006\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivada respecto a $w^{(1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planteamos: \n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial z^{(1)}} \\frac {\\partial z^{(1)}}{\\partial w^{(1)}}\n",
    "$$\n",
    "Donde: \n",
    "$$\n",
    "\\frac {\\partial z^{(1)}}{\\partial w^{(1)}} = x\n",
    "$$\n",
    "Por lo tanto:\n",
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(1)}} = \\frac {\\partial J(\\theta)}{\\partial z^{(1)}} x^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0857381 , -0.16194975],\n",
       "       [-0.01784139,  0.0337004 ],\n",
       "       [ 0.09232211, -0.1743862 ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada_j_respecto_w_1 = derivada_j_respecto_z_1 * x.T\n",
    "derivada_j_respecto_w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac {\\partial J(\\theta)}{\\partial w^{(1)}} = \\begin{pmatrix}\n",
    "0.0857381  & -0.16194975 \\\\\n",
    "-0.01784139 & 0.0337004 \\\\\n",
    "0.09232211 & -0.1743862\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
